{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para poder trabajar con Spark creamos una Sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/19 19:51:39 WARN Utils: Your hostname, DESKTOP-SLEQT56 resolves to a loopback address: 127.0.1.1; using 172.25.13.138 instead (on interface eth0)\n",
      "24/02/19 19:51:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/19 19:51:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .appName('test')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Usando __BASH__ descargamos un archivo de git para analizar.\n",
    "\n",
    "```shell\n",
    "wget url -P directorio_destino\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-19 19:53:23--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-01.csv.gz\n",
      "Resolving github.com (github.com)... 20.201.28.151\n",
      "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/afb2f0a6-bb8b-4958-9818-834bda641e9e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240219T225324Z&X-Amz-Expires=300&X-Amz-Signature=1f886982814c3af4933849c39898af597df874f82d9a6ede464f25774c36f0a0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dyellow_tripdata_2019-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-19 19:53:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/afb2f0a6-bb8b-4958-9818-834bda641e9e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240219T225324Z&X-Amz-Expires=300&X-Amz-Signature=1f886982814c3af4933849c39898af597df874f82d9a6ede464f25774c36f0a0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dyellow_tripdata_2019-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 134445150 (128M) [application/octet-stream]\n",
      "Saving to: ‘../data/yellow_tripdata_2019-01.csv.gz’\n",
      "\n",
      "yellow_tripdata_201 100%[===================>] 128.22M  8.88MB/s    in 14s     \n",
      "\n",
      "2024-02-19 19:53:38 (9.25 MB/s) - ‘../data/yellow_tripdata_2019-01.csv.gz’ saved [134445150/134445150]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2019-01.csv.gz -P ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Si es un archivo del tipo _.gz_ está comprimido asique debemos descomprimirlo.\n",
    "Hay varias formas pero la mas común es con __gunzip__\n",
    "Para mas informacion ver [Como descomprimir en linux](https://www.nexcess.net/help/how-to-decompress-files-in-gzip/#:~:text=A%20GZ%20file%20is%20created,in%20Linux%2FUnix%20operating%20systems.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip  ../data/yellow_tripdata_2019-01.csv.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Contamos la cantidad de registros en el archivo que acabamos de descargar usando bash\n",
    "\n",
    "```shell\n",
    "wc -l archivo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7667793 ../data/yellow_tripdata_2019-01.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../data/yellow_tripdata_2019-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Guardamos el archivo en un _dataframe_ de Spark con _cabecera_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"../data/yellow_tripdata_2019-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Analizando los primeros 5 registros vemos que spark por defecto si no se le especifica un __schema__ toma todas las columnas como __Strings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID='1', tpep_pickup_datetime='2019-01-01 00:46:40', tpep_dropoff_datetime='2019-01-01 00:53:20', passenger_count='1', trip_distance='1.50', RatecodeID='1', store_and_fwd_flag='N', PULocationID='151', DOLocationID='239', payment_type='1', fare_amount='7', extra='0.5', mta_tax='0.5', tip_amount='1.65', tolls_amount='0', improvement_surcharge='0.3', total_amount='9.95', congestion_surcharge=None),\n",
       " Row(VendorID='1', tpep_pickup_datetime='2019-01-01 00:59:47', tpep_dropoff_datetime='2019-01-01 01:18:59', passenger_count='1', trip_distance='2.60', RatecodeID='1', store_and_fwd_flag='N', PULocationID='239', DOLocationID='246', payment_type='1', fare_amount='14', extra='0.5', mta_tax='0.5', tip_amount='1', tolls_amount='0', improvement_surcharge='0.3', total_amount='16.3', congestion_surcharge=None),\n",
       " Row(VendorID='2', tpep_pickup_datetime='2018-12-21 13:48:30', tpep_dropoff_datetime='2018-12-21 13:52:40', passenger_count='3', trip_distance='.00', RatecodeID='1', store_and_fwd_flag='N', PULocationID='236', DOLocationID='236', payment_type='1', fare_amount='4.5', extra='0.5', mta_tax='0.5', tip_amount='0', tolls_amount='0', improvement_surcharge='0.3', total_amount='5.8', congestion_surcharge=None),\n",
       " Row(VendorID='2', tpep_pickup_datetime='2018-11-28 15:52:25', tpep_dropoff_datetime='2018-11-28 15:55:45', passenger_count='5', trip_distance='.00', RatecodeID='1', store_and_fwd_flag='N', PULocationID='193', DOLocationID='193', payment_type='2', fare_amount='3.5', extra='0.5', mta_tax='0.5', tip_amount='0', tolls_amount='0', improvement_surcharge='0.3', total_amount='7.55', congestion_surcharge=None),\n",
       " Row(VendorID='2', tpep_pickup_datetime='2018-11-28 15:56:57', tpep_dropoff_datetime='2018-11-28 15:58:33', passenger_count='5', trip_distance='.00', RatecodeID='2', store_and_fwd_flag='N', PULocationID='193', DOLocationID='193', payment_type='2', fare_amount='52', extra='0', mta_tax='0.5', tip_amount='0', tolls_amount='0', improvement_surcharge='0.3', total_amount='55.55', congestion_surcharge=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Para crear un _schema_ vamos a crear un subset del archivo, con solo 100 registros usando el bash\n",
    "\n",
    "```shell\n",
    "head -n 100 archivo > archivo_destino\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 101 ../data/yellow_tripdata_2019-01.csv > ../data/head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 101  301 9301 ../data/head.csv\n"
     ]
    }
   ],
   "source": [
    "!wc ../data/head.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Usamos Pandas para obtener el nuevo _dataframe_ del que vamos a generar un _schema_.\n",
    "__IMPORTANTE__ No estamos usando _pandas_ para procesar el archivo original, porque es muy grande. SOlo lo usamos para tener una idea del esquema que nos puede ser útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_A tener en cuenta_ La nueva version de pandas tiene un error en los iterables.\n",
    "Cuando creamos un _dataframe_ este no se puede pasar a _spark_.\n",
    "Para solucionar esto hacemos_\n",
    "\n",
    "```python\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items  \n",
    "```\n",
    "\n",
    "[Para mas información seguir este link](https://stackoverflow.com/questions/75926636/databricks-issue-while-creating-spark-data-frame-from-pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:46:40</td>\n",
       "      <td>2019-01-01 00:53:20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>151</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:59:47</td>\n",
       "      <td>2019-01-01 01:18:59</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-21 13:48:30</td>\n",
       "      <td>2018-12-21 13:52:40</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-28 15:52:25</td>\n",
       "      <td>2018-11-28 15:55:45</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-28 15:56:57</td>\n",
       "      <td>2018-11-28 15:58:33</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55.55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2019-01-01 00:46:40   2019-01-01 00:53:20                1   \n",
       "1         1  2019-01-01 00:59:47   2019-01-01 01:18:59                1   \n",
       "2         2  2018-12-21 13:48:30   2018-12-21 13:52:40                3   \n",
       "3         2  2018-11-28 15:52:25   2018-11-28 15:55:45                5   \n",
       "4         2  2018-11-28 15:56:57   2018-11-28 15:58:33                5   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.5           1                  N           151           239   \n",
       "1            2.6           1                  N           239           246   \n",
       "2            0.0           1                  N           236           236   \n",
       "3            0.0           1                  N           193           193   \n",
       "4            0.0           2                  N           193           193   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1          7.0    0.5      0.5        1.65           0.0   \n",
       "1             1         14.0    0.5      0.5        1.00           0.0   \n",
       "2             1          4.5    0.5      0.5        0.00           0.0   \n",
       "3             2          3.5    0.5      0.5        0.00           0.0   \n",
       "4             2         52.0    0.0      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3          9.95                   NaN  \n",
       "1                    0.3         16.30                   NaN  \n",
       "2                    0.3          5.80                   NaN  \n",
       "3                    0.3          7.55                   NaN  \n",
       "4                    0.3         55.55                   NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.iteritems = pd.DataFrame.items  #Necesitamos esto porque sino no podemos crear el DF de spark\n",
    "df_pandas = pd.read_csv('../data/head.csv')\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Creamos el _dataframe_ en Spark con el archivo de los 100 registros de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|              1|          1.5|         1|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                 NaN|\n",
      "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|              1|          2.6|         1|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                 NaN|\n",
      "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|              3|          0.0|         1|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                 NaN|\n",
      "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|              5|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                 NaN|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 Obtenemos una primera versión del schema del _dataframe_de spark y lo copiamos en __vscode__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('tpep_pickup_datetime', StringType(), True), StructField('tpep_dropoff_datetime', StringType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', LongType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Importamos la libreria de __types__ de _pyspark.sql_ que tiene los distintos tipos de datos que necesitamos para abrir el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "types.StructField('VendorID', types.LongType(), True), \n",
    "types.StructField('tpep_pickup_datetime', types.TimestampType(), True), \n",
    "types.StructField('tpep_dropoff_datetime', types.TimestampType(), True), \n",
    "types.StructField('passenger_count', types.LongType(), True), \n",
    "types.StructField('trip_distance', types.DoubleType(), True), \n",
    "types.StructField('RatecodeID', types.LongType(), True), \n",
    "types.StructField('store_and_fwd_flag', types.StringType(), True), \n",
    "types.StructField('PULocationID', types.IntegerType(), True), \n",
    "types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "types.StructField('payment_type', types.LongType(), True), \n",
    "types.StructField('fare_amount', types.DoubleType(), True), \n",
    "types.StructField('extra', types.DoubleType(), True), \n",
    "types.StructField('mta_tax', types.DoubleType(), True), \n",
    "types.StructField('tip_amount', types.DoubleType(), True), \n",
    "types.StructField('tolls_amount', types.DoubleType(), True), \n",
    "types.StructField('improvement_surcharge', types.DoubleType(), True), \n",
    "types.StructField('total_amount', types.DoubleType(), True), \n",
    "types.StructField('congestion_surcharge', types.DoubleType(), True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Volvemos a cergar el archivo original pero ahora indicando el _schema_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .schema(schema=schema) \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"../data/yellow_tripdata_2019-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 1, 1, 0, 46, 40), tpep_dropoff_datetime=datetime.datetime(2019, 1, 1, 0, 53, 20), passenger_count=1, trip_distance=1.5, RatecodeID=1, store_and_fwd_flag='N', PULocationID=151, DOLocationID=239, payment_type=1, fare_amount=7.0, extra=0.5, mta_tax=0.5, tip_amount=1.65, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=9.95, congestion_surcharge=None),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 1, 1, 0, 59, 47), tpep_dropoff_datetime=datetime.datetime(2019, 1, 1, 1, 18, 59), passenger_count=1, trip_distance=2.6, RatecodeID=1, store_and_fwd_flag='N', PULocationID=239, DOLocationID=246, payment_type=1, fare_amount=14.0, extra=0.5, mta_tax=0.5, tip_amount=1.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=16.3, congestion_surcharge=None),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2018, 12, 21, 13, 48, 30), tpep_dropoff_datetime=datetime.datetime(2018, 12, 21, 13, 52, 40), passenger_count=3, trip_distance=0.0, RatecodeID=1, store_and_fwd_flag='N', PULocationID=236, DOLocationID=236, payment_type=1, fare_amount=4.5, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=5.8, congestion_surcharge=None),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2018, 11, 28, 15, 52, 25), tpep_dropoff_datetime=datetime.datetime(2018, 11, 28, 15, 55, 45), passenger_count=5, trip_distance=0.0, RatecodeID=1, store_and_fwd_flag='N', PULocationID=193, DOLocationID=193, payment_type=2, fare_amount=3.5, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=7.55, congestion_surcharge=None),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2018, 11, 28, 15, 56, 57), tpep_dropoff_datetime=datetime.datetime(2018, 11, 28, 15, 58, 33), passenger_count=5, trip_distance=0.0, RatecodeID=2, store_and_fwd_flag='N', PULocationID=193, DOLocationID=193, payment_type=2, fare_amount=52.0, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=55.55, congestion_surcharge=None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2019-01-01 00:46:40|  2019-01-01 00:53:20|              1|          1.5|         1|                 N|         151|         239|           1|        7.0|  0.5|    0.5|      1.65|         0.0|                  0.3|        9.95|                null|\n",
      "|       1| 2019-01-01 00:59:47|  2019-01-01 01:18:59|              1|          2.6|         1|                 N|         239|         246|           1|       14.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        16.3|                null|\n",
      "|       2| 2018-12-21 13:48:30|  2018-12-21 13:52:40|              3|          0.0|         1|                 N|         236|         236|           1|        4.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.8|                null|\n",
      "|       2| 2018-11-28 15:52:25|  2018-11-28 15:55:45|              5|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        7.55|                null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8 Finalmente vamos a Guardar el archivo en formato __parquet__\n",
    "\n",
    "2.8.1 Generamos una repartición del _dataframe_ en n particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8.2 Grabamos el _dataframe_ particionado en formato _parquet_ con el nombre que especificamos.\n",
    "\n",
    "__IMPORTANTE__ _repartition_ es una acción lazzy, lo que quiere decir que no hace nada hastq que se lo indicamos. por ejemplo con _.write()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"../data/pq/2021/01/\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajamos con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Leemos el parquet file que creamos en el paso anterior.\n",
    "Lo interesante de __parquet__ es que este formato tiene el tipo de datos de las columnas.\n",
    "Al saber como guarda la data es más óptimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"../data/pq/2021/01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge\n",
      "1,2019-01-01 00:46:40,2019-01-01 00:53:20,1,1.50,1,N,151,239,1,7,0.5,0.5,1.65,0,0.3,9.95,\n",
      "1,2019-01-01 00:59:47,2019-01-01 01:18:59,1,2.60,1,N,239,246,1,14,0.5,0.5,1,0,0.3,16.3,\n",
      "2,2018-12-21 13:48:30,2018-12-21 13:52:40,3,.00,1,N,236,236,1,4.5,0.5,0.5,0,0,0.3,5.8,\n",
      "2,2018-11-28 15:52:25,2018-11-28 15:55:45,5,.00,1,N,193,193,2,3.5,0.5,0.5,0,0,0.3,7.55,\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ../data/head.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Seleccionamos algunas columnas y le agregamos un filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|PULocationID|DOLocationID|\n",
      "+--------------------+---------------------+------------+------------+\n",
      "| 2018-12-21 13:48:30|  2018-12-21 13:52:40|         236|         236|\n",
      "| 2018-11-28 15:52:25|  2018-11-28 15:55:45|         193|         193|\n",
      "| 2018-11-28 15:56:57|  2018-11-28 15:58:33|         193|         193|\n",
      "| 2018-11-28 16:25:49|  2018-11-28 16:28:26|         193|         193|\n",
      "| 2018-11-28 16:29:37|  2018-11-28 16:33:43|         193|         193|\n",
      "+--------------------+---------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('tpep_pickup_datetime','tpep_dropoff_datetime','PULocationID','DOLocationID')\\\n",
    "    .filter(df.VendorID==2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Ejemplo de transformacion .withColumn() con funcion de spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+---------------+\n",
      "|VendorID|tpep_pickup_datetime|passenger_count|horarioDePickUp|\n",
      "+--------+--------------------+---------------+---------------+\n",
      "|       1| 2019-01-01 00:46:40|              1|     2019-01-01|\n",
      "|       1| 2019-01-01 00:59:47|              1|     2019-01-01|\n",
      "|       2| 2018-12-21 13:48:30|              3|     2018-12-21|\n",
      "|       2| 2018-11-28 15:52:25|              5|     2018-11-28|\n",
      "|       2| 2018-11-28 15:56:57|              5|     2018-11-28|\n",
      "+--------+--------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.withColumn('horarioDePickUp', F.to_date(df.tpep_pickup_datetime))\\\n",
    "    .select('VendorID','tpep_pickup_datetime','passenger_count','horarioDePickUp')\\\n",
    "        .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Definimos nuestra propia funcion (UDF) para usarla en una acción .withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def hace_algo(valor):\n",
    "    if valor >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "udf_pass_bool = F.udf(hace_algo, returnType= types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|passenger_count|passenger_count_boll|\n",
      "+--------+--------------------+---------------+--------------------+\n",
      "|       1| 2019-01-01 00:46:40|              1|                   0|\n",
      "|       1| 2019-01-01 00:59:47|              1|                   0|\n",
      "|       2| 2018-12-21 13:48:30|              3|                   1|\n",
      "|       2| 2018-11-28 15:52:25|              5|                   1|\n",
      "+--------+--------------------+---------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('passenger_count_boll',udf_pass_bool(df.passenger_count) )\\\n",
    "    .select('VendorID','tpep_pickup_datetime','passenger_count', 'passenger_count_boll')\\\n",
    "        .show(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
